---
title: 'Note on beneficial AI and urban planning'
date: 2020-01-04
tags:
  - urbanism
  - AI
  - spatial heterogeneity
  - data science
header:
  image: /images/gini/gini_cover.jpg
excerpt: 'AI, Spatial Heterogeneity, Urbanism'
mathjax: 'true'
---

This is a short note reflecting on the implications of the current AI paradigms on the future of urban planning.

Let's say it's 2050 and you have an AI planner assistant system helping you to achieve certain targets in the new city masterplan. For instance, one of the objectives as part of the urban economic policy for the masterplan is to achieve a low Gini coefficient (i.e. low inequality).

If the current AI paradigm, namely that which is called the "standard model" where an algorithm optimizes a fixed objective (e.g. minimizes a defined loss function), becomes the cornerstone for future developments, we will have a problem: a machine more "intelligent" than us will achieve the "fixed objective", or the purpose we put in the machine, using whatever means and won't care of any side-effects. 
For example*, the spatial distribution of some values in London, shown in the left of the below image has the **exact same** Gini coefficient as its spatial reshuffling shown in the right.
While the measure of inequality is the same in both cases, the reshuffled version clearly shows high vs. low values spatially segregated. In the case of income this would imply achieving the fixed objective though what urban planners call exclusionary rather than inclusionary masterplanning.

<img src="{{ site.url }}{{ site.baseurl }}/images/gini/shuffled.jpg" alt="London parking demand reshuffled distributions">

While interpretable machine learning is gaining momentum nowadays, I find the so called apprenticeship learning via inverse reinforcement learning described by Stuart Russell very promising in making sure we are in control.
In a traditional RL setting, the goal is to learn a behavior that maximizes a predefined reward function. Inverse reinforcement learning (IRL), turns the problem on its head: it tries to extract an approximation of the reward function given the observed behavior of an agent.

Since humans often have irrational and inconsistent preferences, the IRL system will have a hard time learning them, and the only way to deal with it 



*A more amusing example: an "intelligent" vacuum cleaner (with the fixed objective to make sure there is no dust in the house)  might at some point come up with another way to fight dust: get rid of its source! This is clearly something you don't want to happen without you being in control since a good part of the indoor dust comes from human hair and skin cells.
